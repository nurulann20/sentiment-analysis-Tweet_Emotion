# -*- coding: utf-8 -*-
"""Tambahan_Training & Testing Menggunakan pyspark (Khusus NB).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mdh8UFTXuctWDkp7Y2P-5HAaiS8caU7n
"""

pip install pyspark

"""# Naive Bayes PySpark"""

import pandas as pd
from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, Tokenizer, HashingTF, IDF, IndexToString
from pyspark.ml.classification import NaiveBayes
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix

# Create SparkSession
spark = SparkSession.builder.appName("NaiveBayes with PySpark").getOrCreate()

# Read data from CSV
data = spark.read.csv("tweet_emotions_preprocessed.csv", header=True, inferSchema=True)

# Show data schema
data.printSchema()

# Show some data
data.show()

# Select columns to be used as features and labels
data_spark = data.select("content", "sentiment")

# Index labels
label_indexer = StringIndexer(inputCol="sentiment", outputCol="label")

# Split data
train_data, test_data = data_spark.randomSplit([0.8, 0.2], seed=42)

# Feature extraction using TF-IDF
tokenizer = Tokenizer(inputCol="content", outputCol="words")
hashingTF = HashingTF(inputCol="words", outputCol="raw_features", numFeatures=1000)
idf = IDF(inputCol="raw_features", outputCol="features")

# Create NaiveBayes model
nb = NaiveBayes(featuresCol="features")

# Create pipeline
pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, label_indexer, nb])

# Hyperparameter tuning with CrossValidator
paramGrid = ParamGridBuilder() \
    .addGrid(nb.smoothing, [0.0, 0.5, 1.0]) \
    .build()

crossval = CrossValidator(estimator=pipeline,
                          estimatorParamMaps=paramGrid,
                          evaluator=MulticlassClassificationEvaluator(),
                          numFolds=5)

# Train model
cvModel = crossval.fit(train_data)

# Save StringIndexer model
string_indexer_model = label_indexer.fit(train_data)

# Evaluate model
predictions = cvModel.transform(test_data)

# Convert label index to original label
label_converter = IndexToString(inputCol="prediction", outputCol="predicted_sentiment", labels=string_indexer_model.labels)

# Transform predictions using label converter
predictions = label_converter.transform(predictions)
predictions.show()

# Extract true labels and predictions from Spark DataFrame and convert to Python lists
y_true = predictions.select("sentiment").rdd.map(lambda row: row[0]).collect()
y_pred = predictions.select("predicted_sentiment").rdd.map(lambda row: row[0]).collect()

# Compute evaluation metrics using scikit-learn
classification_rep = classification_report(y_true, y_pred)
accuracy = accuracy_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred, average='weighted')
recall = recall_score(y_true, y_pred, average='weighted')
precision = precision_score(y_true, y_pred, average='weighted')

# Compute confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred)

# Convert confusion matrix to Pandas DataFrame
conf_matrix_df = pd.DataFrame(conf_matrix, index=string_indexer_model.labels, columns=string_indexer_model.labels)

# Display evaluation results
print("Classification Report:")
print(classification_rep)
print("Accuracy:", accuracy)
print("F1 Score:", f1)
print("Recall:", recall)
print("Precision:", precision)

# Stop SparkSession
spark.stop()

import seaborn as sns
import matplotlib.pyplot as plt
# Buat heatmap menggunakan Seaborn
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix_df, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

