# -*- coding: utf-8 -*-
"""Tambahan_Training & Testing Mengunakaan Word2vec.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1saCkaAhl0psfgEs3kefiqdiqyeaJKyvr

# Percobaan Training dan Testing menggunakan Word2Vec

# Import
"""

import numpy as np
import pandas as pd
import gensim
import nltk
import re
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

"""# Data Preprocessing"""

# Pilih kolom yang akan digunakan sebagai fitur dan label
x = data['content']
y = data['sentiment']

# We'll write a function which will clean the text and prepare it.
def cleanText(text):
    cleaned = re.sub("[^a-zA-Z0-9']"," ",text)
    lowered = cleaned.lower()
    return lowered.strip()

x,y = np.asarray(data["content"]),np.asarray(data["sentiment"])

x_cleaned = [cleanText(t) for t in x]
x_cleaned[:4]

# Mengganti class dari text ke integer
label_map = {cat:index for index,cat in enumerate(np.unique(y))}
y_prep = np.asarray([label_map[l] for l in y])

label_map

"""# Training Word Embeddings


"""

# menjadikan kalimat ke list berisi penggalan per kata
x_tokenized = [[w for w in sentence.split(" ") if w != ""] for sentence in x_cleaned]
x_tokenized[0]

# membuat model
import time

start = time.time()

model = gensim.models.Word2Vec(x_tokenized,
                 vector_size=100
                 # Size is the length of our vector.
                )

end = round(time.time()-start,2)
print("This process took",end,"seconds.")

model.wv.most_similar("free")

"""# Writing A Class To Create Sequences
Our model is ready, but we need a class to convert texts to create word embedding sequences
"""

class Sequencer():

    def __init__(self,
                 all_words,
                 max_words,
                 seq_len,
                 embedding_matrix
                ):

        self.seq_len = seq_len
        self.embed_matrix = embedding_matrix
        """
        temp_vocab = Vocab which has all the unique words
        self.vocab = Our last vocab which has only most used N words.

        """
        temp_vocab = list(set(all_words))
        self.vocab = []
        self.word_cnts = {}
        """
        Now we'll create a hash map (dict) which includes words and their occurencies
        """
        for word in temp_vocab:
            # 0 does not have a meaning, you can add the word to the list
            # or something different.
            count = len([0 for w in all_words if w == word])
            self.word_cnts[word] = count
            counts = list(self.word_cnts.values())
            indexes = list(range(len(counts)))

        # Now we'll sort counts and while sorting them also will sort indexes.
        # We'll use those indexes to find most used N word.
        cnt = 0
        while cnt + 1 != len(counts):
            cnt = 0
            for i in range(len(counts)-1):
                if counts[i] < counts[i+1]:
                    counts[i+1],counts[i] = counts[i],counts[i+1]
                    indexes[i],indexes[i+1] = indexes[i+1],indexes[i]
                else:
                    cnt += 1

        for ind in indexes[:max_words]:
            self.vocab.append(temp_vocab[ind])

    def textToVector(self,text):
        # First we need to split the text into its tokens and learn the length
        # If length is shorter than the max len we'll add some spaces (100D vectors which has only zero values)
        # If it's longer than the max len we'll trim from the end.
        tokens = text.split()
        len_v = len(tokens)-1 if len(tokens) < self.seq_len else self.seq_len-1
        vec = []
        for tok in tokens[:len_v]:
            try:
                vec.append(self.embed_matrix[tok])
            except Exception as E:
                pass

        last_pieces = self.seq_len - len(vec)
        for i in range(last_pieces):
            vec.append(np.zeros(100,))

        return np.asarray(vec).flatten()

"""* Our class is ready, let's take a last look at that.
    1. In constructor function our class takes 4 parameters: all_words,max_words,seq_length,embedding_matrix
        * All Words = This means give your all dataset in a list format which contains all tokens (not list of lists (sentences) concatenate all the sentences).
        * Max Words = If your dataset has a lot of unique words you might want to limit the number of words. This parameter will be used in finding most used N (max_words) word.
        * Sequence Length = In machine learning our dataset's number of variable has to be specified. But in real life each sentence might has a different length. In order to prevent this problem we'll determine a length and adapt our sentences to that length.
"""

sequencer = Sequencer(all_words = [token for seq in x_tokenized for token in seq],
              max_words = 1200,
              seq_len = 15,
              embedding_matrix = model.wv
             )

test_vec = sequencer.textToVector("i am in love with you")
test_vec

test_vec.shape

"""# PCA (Principal Component Analysis)

Everything looks fine, but as you see each vector for a sentence has 1500 elements and it'll consume a lot of time to train a Support Vector Machine Classifier on this.

In order to prevent this problem, we'll use the power of Statistics, Principal Component Analysis.
Principal Component Analysis is a way to reduce dimension of vectors. It maximizes the variance and creates N components.
"""

# But before creating a PCA model using scikit-learn let's create
# vectors for our each vector
x_vecs = np.asarray([sequencer.textToVector(" ".join(seq)) for seq in x_tokenized])
print(x_vecs.shape)

from sklearn.decomposition import PCA
pca_model = PCA(n_components=50)
pca_model.fit(x_vecs)
print("Sum of variance ratios: ",sum(pca_model.explained_variance_ratio_))

"""* We've said we want 50 components and with those 50 component we can protect our data's %99.
* Stats look nice, so let's use transform function and reduce dimension.

"""

x_comps = pca_model.transform(x_vecs)
x_comps.shape

"""* Everything is nice about the data, let's split our data to train and test set."""

x_train,x_test,y_train,y_test = train_test_split(x_comps,y_prep,test_size=0.2,random_state=42)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""# Support Vector Machine Classifier


"""

start = time.time()

svm_classifier = SVC()
svm_classifier.fit(x_train,y_train)

end = time.time()
process = round(end-start,2)
print("Support Vector Machine Classifier has fitted, this process took {} seconds".format(process))

"""* Selanjutnya mengecek akurasi dari model pelatihan svm ke data uji"""

svm_classifier.score(x_test,y_test)

# Algoritma lain
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB

gnb = GaussianNB()
gnb.fit(x_train,y_train)
print("Score of GaussianNB",gnb.score(x_test,y_test))

bnb = BernoulliNB()
bnb.fit(x_train,y_train)
print("Score of BernoulliNB",bnb.score(x_test,y_test))

# MultinomialNB dengan MinMaxScaler
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(x_train)
X_test_scaled = scaler.transform(x_test)

mnb = MultinomialNB()
mnb.fit(X_train_scaled,y_train)
print("Score of MultinomialNB",mnb.score(X_test_scaled,y_test))

"""* Hasil akurasi yang didapat masih rendah (akurasi < 0,3)

"""